Layer 1 - Accuracy: 63.47%
              precision    recall  f1-score   support

         0.0       0.92      0.96      0.94      6418
         1.0       0.34      0.11      0.17       183
         2.0       0.74      0.87      0.80      2079
         3.0       0.66      0.57      0.61       412
         4.0       0.50      0.49      0.50      1996
         5.0       0.61      0.74      0.67      1674
         6.0       0.47      0.60      0.52       878
         7.0       0.52      0.47      0.49       182
         8.0       0.67      0.51      0.58      1007
         9.0       0.56      0.50      0.53       527
        10.0       0.63      0.60      0.61       782
        11.0       0.67      0.85      0.75       422
        12.0       0.67      0.66      0.67       600
        13.0       0.54      0.44      0.49       317
        14.0       0.69      0.63      0.66       662
        15.0       0.55      0.52      0.53       397
        16.0       0.67      0.33      0.44       278
        17.0       0.58      0.29      0.39        73
        18.0       0.47      0.32      0.38       148
        19.0       0.48      0.30      0.37      1271
        20.0       0.80      0.32      0.46        25
        21.0       0.56      0.32      0.40       294
        22.0       0.35      0.18      0.24       111
        23.0       0.67      0.68      0.67       647
        24.0       0.50      0.58      0.54       528
        25.0       0.53      0.74      0.62       926
        26.0       0.41      0.59      0.49       482
        27.0       0.42      0.45      0.44      1394
        28.0       0.64      0.62      0.63       865
        29.0       0.57      0.56      0.57       604
        30.0       0.65      0.69      0.67      1202
        31.0       0.43      0.49      0.46      2107
        32.0       0.47      0.57      0.51       640
        33.0       0.61      0.40      0.49       546
        34.0       0.46      0.32      0.38       282
        35.0       0.65      0.45      0.53       736
        36.0       0.58      0.58      0.58       958
        37.0       0.62      0.49      0.55      1143
        38.0       0.29      0.25      0.27       210
        39.0       0.66      0.74      0.69      1219
        40.0       0.48      0.31      0.38       873

    accuracy                           0.63     36098
   macro avg       0.57      0.51      0.53     36098
weighted avg       0.63      0.63      0.63     36098


Layer 2 - Accuracy: 71.47%
              precision    recall  f1-score   support

         0.0       0.93      0.97      0.95      6418
         1.0       0.42      0.28      0.34       183
         2.0       0.81      0.88      0.84      2079
         3.0       0.74      0.72      0.73       412
         4.0       0.61      0.62      0.61      1996
         5.0       0.76      0.78      0.77      1674
         6.0       0.72      0.76      0.74       878
         7.0       0.61      0.61      0.61       182
         8.0       0.75      0.64      0.69      1007
         9.0       0.57      0.66      0.61       527
        10.0       0.67      0.79      0.72       782
        11.0       0.81      0.82      0.81       422
        12.0       0.75      0.77      0.76       600
        13.0       0.64      0.69      0.66       317
        14.0       0.69      0.71      0.70       662
        15.0       0.63      0.70      0.67       397
        16.0       0.64      0.54      0.58       278
        17.0       0.64      0.37      0.47        73
        18.0       0.60      0.49      0.54       148
        19.0       0.53      0.48      0.51      1271
        20.0       0.67      0.40      0.50        25
        21.0       0.65      0.61      0.63       294
        22.0       0.45      0.41      0.43       111
        23.0       0.79      0.77      0.78       647
        24.0       0.75      0.66      0.70       528
        25.0       0.62      0.73      0.67       926
        26.0       0.55      0.58      0.57       482
        27.0       0.51      0.46      0.48      1394
        28.0       0.77      0.77      0.77       865
        29.0       0.69      0.71      0.70       604
        30.0       0.77      0.71      0.74      1202
        31.0       0.55      0.48      0.52      2107
        32.0       0.71      0.64      0.67       640
        33.0       0.62      0.68      0.65       546
        34.0       0.64      0.56      0.60       282
        35.0       0.61      0.62      0.61       736
        36.0       0.66      0.62      0.64       958
        37.0       0.67      0.67      0.67      1143
        38.0       0.47      0.44      0.45       210
        39.0       0.74      0.72      0.73      1219
        40.0       0.48      0.52      0.50       873

    accuracy                           0.71     36098
   macro avg       0.66      0.64      0.64     36098
weighted avg       0.71      0.71      0.71     36098


Layer 3 - Accuracy: 78.07%
              precision    recall  f1-score   support

         0.0       0.96      0.97      0.96      6418
         1.0       0.46      0.29      0.36       183
         2.0       0.91      0.82      0.86      2079
         3.0       0.83      0.73      0.78       412
         4.0       0.72      0.67      0.69      1996
         5.0       0.81      0.80      0.80      1674
         6.0       0.87      0.86      0.87       878
         7.0       0.77      0.60      0.68       182
         8.0       0.67      0.85      0.75      1007
         9.0       0.71      0.71      0.71       527
        10.0       0.82      0.85      0.84       782
        11.0       0.85      0.89      0.87       422
        12.0       0.78      0.87      0.82       600
        13.0       0.85      0.73      0.79       317
        14.0       0.80      0.78      0.79       662
        15.0       0.73      0.81      0.77       397
        16.0       0.68      0.78      0.73       278
        17.0       0.85      0.62      0.71        73
        18.0       0.73      0.64      0.68       148
        19.0       0.55      0.68      0.60      1271
        20.0       0.93      0.56      0.70        25
        21.0       0.83      0.81      0.82       294
        22.0       0.64      0.60      0.62       111
        23.0       0.88      0.86      0.87       647
        24.0       0.82      0.82      0.82       528
        25.0       0.71      0.75      0.73       926
        26.0       0.64      0.72      0.68       482
        27.0       0.61      0.54      0.57      1394
        28.0       0.82      0.86      0.84       865
        29.0       0.77      0.79      0.78       604
        30.0       0.80      0.81      0.81      1202
        31.0       0.60      0.62      0.61      2107
        32.0       0.86      0.71      0.77       640
        33.0       0.76      0.72      0.74       546
        34.0       0.79      0.74      0.76       282
        35.0       0.77      0.66      0.71       736
        36.0       0.72      0.71      0.71       958
        37.0       0.77      0.69      0.73      1143
        38.0       0.61      0.64      0.63       210
        39.0       0.76      0.86      0.80      1219
        40.0       0.62      0.58      0.60       873

    accuracy                           0.78     36098
   macro avg       0.76      0.73      0.74     36098
weighted avg       0.78      0.78      0.78     36098


Layer 4 - Accuracy: 81.94%
              precision    recall  f1-score   support

         0.0       0.95      0.97      0.96      6418
         1.0       0.64      0.34      0.45       183
         2.0       0.88      0.93      0.90      2079
         3.0       0.82      0.87      0.84       412
         4.0       0.75      0.80      0.78      1996
         5.0       0.81      0.88      0.84      1674
         6.0       0.88      0.90      0.89       878
         7.0       0.79      0.66      0.72       182
         8.0       0.85      0.81      0.83      1007
         9.0       0.66      0.80      0.72       527
        10.0       0.84      0.90      0.87       782
        11.0       0.89      0.91      0.90       422
        12.0       0.87      0.81      0.84       600
        13.0       0.88      0.77      0.82       317
        14.0       0.87      0.85      0.86       662
        15.0       0.85      0.78      0.81       397
        16.0       0.83      0.74      0.78       278
        17.0       0.87      0.73      0.79        73
        18.0       0.76      0.72      0.74       148
        19.0       0.74      0.59      0.65      1271
        20.0       1.00      0.72      0.84        25
        21.0       0.89      0.83      0.86       294
        22.0       0.79      0.64      0.71       111
        23.0       0.89      0.89      0.89       647
        24.0       0.83      0.83      0.83       528
        25.0       0.80      0.75      0.78       926
        26.0       0.78      0.60      0.68       482
        27.0       0.69      0.59      0.64      1394
        28.0       0.91      0.84      0.88       865
        29.0       0.73      0.91      0.81       604
        30.0       0.81      0.86      0.84      1202
        31.0       0.73      0.60      0.66      2107
        32.0       0.85      0.84      0.85       640
        33.0       0.76      0.82      0.79       546
        34.0       0.88      0.75      0.81       282
        35.0       0.75      0.73      0.74       736
        36.0       0.73      0.79      0.76       958
        37.0       0.72      0.82      0.77      1143
        38.0       0.81      0.63      0.71       210
        39.0       0.78      0.87      0.82      1219
        40.0       0.62      0.69      0.65       873

    accuracy                           0.82     36098
   macro avg       0.81      0.77      0.79     36098
weighted avg       0.82      0.82      0.82     36098


Layer 5 - Accuracy: 83.17%
              precision    recall  f1-score   support

         0.0       0.96      0.97      0.96      6418
         1.0       0.62      0.45      0.52       183
         2.0       0.89      0.92      0.91      2079
         3.0       0.81      0.84      0.83       412
         4.0       0.75      0.83      0.79      1996
         5.0       0.84      0.86      0.85      1674
         6.0       0.89      0.91      0.90       878
         7.0       0.82      0.66      0.74       182
         8.0       0.88      0.78      0.82      1007
         9.0       0.71      0.80      0.75       527
        10.0       0.85      0.91      0.88       782
        11.0       0.89      0.89      0.89       422
        12.0       0.87      0.84      0.85       600
        13.0       0.88      0.84      0.86       317
        14.0       0.85      0.88      0.87       662
        15.0       0.84      0.83      0.83       397
        16.0       0.86      0.79      0.82       278
        17.0       0.93      0.75      0.83        73
        18.0       0.80      0.72      0.76       148
        19.0       0.75      0.63      0.69      1271
        20.0       0.88      0.60      0.71        25
        21.0       0.90      0.85      0.88       294
        22.0       0.71      0.69      0.70       111
        23.0       0.88      0.92      0.90       647
        24.0       0.85      0.85      0.85       528
        25.0       0.75      0.82      0.78       926
        26.0       0.80      0.68      0.73       482
        27.0       0.71      0.61      0.66      1394
        28.0       0.88      0.89      0.89       865
        29.0       0.84      0.85      0.85       604
        30.0       0.84      0.85      0.84      1202
        31.0       0.62      0.76      0.68      2107
        32.0       0.88      0.85      0.87       640
        33.0       0.84      0.81      0.82       546
        34.0       0.80      0.87      0.83       282
        35.0       0.83      0.73      0.78       736
        36.0       0.80      0.77      0.78       958
        37.0       0.79      0.76      0.77      1143
        38.0       0.83      0.63      0.72       210
        39.0       0.83      0.86      0.84      1219
        40.0       0.76      0.59      0.66       873

    accuracy                           0.83     36098
   macro avg       0.82      0.79      0.80     36098
weighted avg       0.83      0.83      0.83     36098


Layer 6 - Accuracy: 83.46%
              precision    recall  f1-score   support

         0.0       0.94      0.97      0.96      6418
         1.0       0.58      0.44      0.50       183
         2.0       0.91      0.91      0.91      2079
         3.0       0.83      0.86      0.84       412
         4.0       0.78      0.79      0.78      1996
         5.0       0.87      0.82      0.84      1674
         6.0       0.87      0.93      0.90       878
         7.0       0.72      0.74      0.73       182
         8.0       0.81      0.86      0.83      1007
         9.0       0.78      0.71      0.74       527
        10.0       0.86      0.91      0.89       782
        11.0       0.91      0.90      0.90       422
        12.0       0.84      0.87      0.85       600
        13.0       0.86      0.83      0.84       317
        14.0       0.85      0.86      0.85       662
        15.0       0.84      0.87      0.85       397
        16.0       0.83      0.79      0.81       278
        17.0       0.91      0.71      0.80        73
        18.0       0.80      0.70      0.74       148
        19.0       0.75      0.66      0.70      1271
        20.0       0.94      0.64      0.76        25
        21.0       0.95      0.85      0.90       294
        22.0       0.80      0.65      0.72       111
        23.0       0.89      0.90      0.90       647
        24.0       0.86      0.84      0.85       528
        25.0       0.79      0.81      0.80       926
        26.0       0.76      0.74      0.75       482
        27.0       0.69      0.66      0.68      1394
        28.0       0.88      0.88      0.88       865
        29.0       0.81      0.88      0.85       604
        30.0       0.85      0.86      0.85      1202
        31.0       0.68      0.73      0.70      2107
        32.0       0.86      0.86      0.86       640
        33.0       0.79      0.82      0.80       546
        34.0       0.85      0.85      0.85       282
        35.0       0.86      0.73      0.79       736
        36.0       0.84      0.74      0.78       958
        37.0       0.75      0.81      0.78      1143
        38.0       0.81      0.67      0.73       210
        39.0       0.85      0.81      0.83      1219
        40.0       0.70      0.71      0.71       873

    accuracy                           0.83     36098
   macro avg       0.82      0.79      0.81     36098
weighted avg       0.83      0.83      0.83     36098


Layer 7 - Accuracy: 83.73%
              precision    recall  f1-score   support

         0.0       0.95      0.97      0.96      6418
         1.0       0.63      0.36      0.45       183
         2.0       0.86      0.94      0.90      2079
         3.0       0.85      0.81      0.83       412
         4.0       0.77      0.80      0.79      1996
         5.0       0.79      0.89      0.84      1674
         6.0       0.91      0.89      0.90       878
         7.0       0.77      0.71      0.74       182
         8.0       0.82      0.81      0.82      1007
         9.0       0.84      0.62      0.71       527
        10.0       0.84      0.92      0.88       782
        11.0       0.89      0.91      0.90       422
        12.0       0.83      0.87      0.85       600
        13.0       0.89      0.76      0.82       317
        14.0       0.89      0.85      0.87       662
        15.0       0.85      0.86      0.86       397
        16.0       0.87      0.73      0.80       278
        17.0       0.89      0.75      0.81        73
        18.0       0.71      0.74      0.72       148
        19.0       0.77      0.67      0.71      1271
        20.0       0.82      0.72      0.77        25
        21.0       0.91      0.84      0.87       294
        22.0       0.77      0.68      0.72       111
        23.0       0.89      0.93      0.91       647
        24.0       0.87      0.85      0.86       528
        25.0       0.75      0.84      0.80       926
        26.0       0.67      0.77      0.72       482
        27.0       0.70      0.67      0.68      1394
        28.0       0.88      0.89      0.88       865
        29.0       0.82      0.92      0.87       604
        30.0       0.86      0.87      0.86      1202
        31.0       0.74      0.66      0.70      2107
        32.0       0.87      0.85      0.86       640
        33.0       0.80      0.84      0.82       546
        34.0       0.89      0.77      0.82       282
        35.0       0.83      0.74      0.78       736
        36.0       0.80      0.79      0.79       958
        37.0       0.79      0.79      0.79      1143
        38.0       0.85      0.73      0.79       210
        39.0       0.85      0.86      0.85      1219
        40.0       0.72      0.73      0.72       873

    accuracy                           0.84     36098
   macro avg       0.82      0.80      0.81     36098
weighted avg       0.84      0.84      0.84     36098


Layer 8 - Accuracy: 83.51%
              precision    recall  f1-score   support

         0.0       0.95      0.97      0.96      6418
         1.0       0.62      0.38      0.47       183
         2.0       0.91      0.92      0.92      2079
         3.0       0.82      0.85      0.84       412
         4.0       0.80      0.79      0.80      1996
         5.0       0.84      0.86      0.85      1674
         6.0       0.90      0.91      0.90       878
         7.0       0.78      0.70      0.74       182
         8.0       0.85      0.82      0.84      1007
         9.0       0.75      0.73      0.74       527
        10.0       0.90      0.86      0.88       782
        11.0       0.87      0.91      0.89       422
        12.0       0.89      0.83      0.86       600
        13.0       0.86      0.84      0.85       317
        14.0       0.83      0.90      0.86       662
        15.0       0.89      0.85      0.87       397
        16.0       0.82      0.78      0.80       278
        17.0       0.87      0.63      0.73        73
        18.0       0.82      0.68      0.75       148
        19.0       0.70      0.70      0.70      1271
        20.0       0.77      0.68      0.72        25
        21.0       0.94      0.85      0.89       294
        22.0       0.85      0.64      0.73       111
        23.0       0.91      0.90      0.90       647
        24.0       0.87      0.84      0.86       528
        25.0       0.74      0.81      0.77       926
        26.0       0.63      0.79      0.70       482
        27.0       0.79      0.54      0.64      1394
        28.0       0.86      0.91      0.89       865
        29.0       0.87      0.87      0.87       604
        30.0       0.85      0.87      0.86      1202
        31.0       0.67      0.73      0.70      2107
        32.0       0.88      0.82      0.85       640
        33.0       0.87      0.78      0.82       546
        34.0       0.80      0.84      0.82       282
        35.0       0.81      0.76      0.78       736
        36.0       0.73      0.84      0.78       958
        37.0       0.76      0.78      0.77      1143
        38.0       0.79      0.78      0.78       210
        39.0       0.81      0.88      0.84      1219
        40.0       0.75      0.66      0.70       873

    accuracy                           0.84     36098
   macro avg       0.82      0.79      0.80     36098
weighted avg       0.84      0.84      0.83     36098


Layer 9 - Accuracy: 84.14%
              precision    recall  f1-score   support

         0.0       0.94      0.97      0.96      6418
         1.0       0.61      0.43      0.50       183
         2.0       0.89      0.93      0.91      2079
         3.0       0.84      0.88      0.86       412
         4.0       0.79      0.81      0.80      1996
         5.0       0.83      0.87      0.85      1674
         6.0       0.91      0.91      0.91       878
         7.0       0.79      0.70      0.74       182
         8.0       0.85      0.86      0.85      1007
         9.0       0.70      0.70      0.70       527
        10.0       0.91      0.89      0.90       782
        11.0       0.90      0.89      0.90       422
        12.0       0.86      0.84      0.85       600
        13.0       0.85      0.85      0.85       317
        14.0       0.88      0.85      0.86       662
        15.0       0.84      0.86      0.85       397
        16.0       0.87      0.81      0.84       278
        17.0       0.93      0.77      0.84        73
        18.0       0.78      0.76      0.77       148
        19.0       0.81      0.64      0.71      1271
        20.0       0.94      0.64      0.76        25
        21.0       0.91      0.86      0.88       294
        22.0       0.84      0.75      0.79       111
        23.0       0.91      0.90      0.91       647
        24.0       0.85      0.88      0.87       528
        25.0       0.84      0.74      0.79       926
        26.0       0.63      0.79      0.70       482
        27.0       0.73      0.66      0.70      1394
        28.0       0.86      0.91      0.88       865
        29.0       0.79      0.92      0.85       604
        30.0       0.87      0.84      0.86      1202
        31.0       0.75      0.67      0.71      2107
        32.0       0.88      0.85      0.86       640
        33.0       0.82      0.80      0.81       546
        34.0       0.84      0.88      0.86       282
        35.0       0.86      0.72      0.78       736
        36.0       0.77      0.83      0.80       958
        37.0       0.71      0.86      0.78      1143
        38.0       0.86      0.76      0.81       210
        39.0       0.80      0.89      0.84      1219
        40.0       0.76      0.70      0.73       873

    accuracy                           0.84     36098
   macro avg       0.83      0.81      0.82     36098
weighted avg       0.84      0.84      0.84     36098


Layer 10 - Accuracy: 83.55%
              precision    recall  f1-score   support

         0.0       0.93      0.97      0.95      6418
         1.0       0.65      0.45      0.53       183
         2.0       0.89      0.91      0.90      2079
         3.0       0.86      0.84      0.85       412
         4.0       0.78      0.79      0.78      1996
         5.0       0.84      0.87      0.85      1674
         6.0       0.89      0.90      0.90       878
         7.0       0.79      0.65      0.71       182
         8.0       0.83      0.81      0.82      1007
         9.0       0.77      0.72      0.74       527
        10.0       0.89      0.88      0.89       782
        11.0       0.90      0.90      0.90       422
        12.0       0.85      0.83      0.84       600
        13.0       0.89      0.82      0.85       317
        14.0       0.84      0.86      0.85       662
        15.0       0.79      0.85      0.82       397
        16.0       0.87      0.76      0.81       278
        17.0       0.95      0.74      0.83        73
        18.0       0.81      0.68      0.74       148
        19.0       0.73      0.66      0.69      1271
        20.0       0.79      0.60      0.68        25
        21.0       0.92      0.85      0.88       294
        22.0       0.86      0.72      0.78       111
        23.0       0.89      0.90      0.89       647
        24.0       0.88      0.83      0.85       528
        25.0       0.77      0.84      0.81       926
        26.0       0.68      0.79      0.73       482
        27.0       0.71      0.70      0.70      1394
        28.0       0.89      0.87      0.88       865
        29.0       0.82      0.87      0.84       604
        30.0       0.84      0.86      0.85      1202
        31.0       0.71      0.71      0.71      2107
        32.0       0.86      0.84      0.85       640
        33.0       0.85      0.82      0.83       546
        34.0       0.88      0.79      0.84       282
        35.0       0.81      0.72      0.76       736
        36.0       0.77      0.82      0.79       958
        37.0       0.83      0.78      0.80      1143
        38.0       0.84      0.72      0.78       210
        39.0       0.83      0.86      0.85      1219
        40.0       0.75      0.67      0.71       873

    accuracy                           0.84     36098
   macro avg       0.83      0.79      0.81     36098
weighted avg       0.83      0.84      0.83     36098


Layer 11 - Accuracy: 82.39%
              precision    recall  f1-score   support

         0.0       0.93      0.97      0.95      6418
         1.0       0.65      0.42      0.51       183
         2.0       0.87      0.90      0.88      2079
         3.0       0.85      0.82      0.84       412
         4.0       0.78      0.76      0.77      1996
         5.0       0.85      0.84      0.84      1674
         6.0       0.87      0.88      0.88       878
         7.0       0.74      0.64      0.69       182
         8.0       0.81      0.80      0.80      1007
         9.0       0.73      0.77      0.75       527
        10.0       0.86      0.86      0.86       782
        11.0       0.89      0.90      0.89       422
        12.0       0.83      0.83      0.83       600
        13.0       0.85      0.84      0.84       317
        14.0       0.85      0.85      0.85       662
        15.0       0.77      0.82      0.79       397
        16.0       0.85      0.78      0.81       278
        17.0       0.94      0.63      0.75        73
        18.0       0.80      0.67      0.73       148
        19.0       0.68      0.69      0.69      1271
        20.0       0.78      0.56      0.65        25
        21.0       0.87      0.85      0.86       294
        22.0       0.78      0.73      0.75       111
        23.0       0.87      0.88      0.88       647
        24.0       0.83      0.85      0.84       528
        25.0       0.79      0.81      0.80       926
        26.0       0.73      0.70      0.72       482
        27.0       0.73      0.65      0.69      1394
        28.0       0.85      0.86      0.85       865
        29.0       0.82      0.88      0.85       604
        30.0       0.86      0.82      0.84      1202
        31.0       0.69      0.70      0.69      2107
        32.0       0.84      0.82      0.83       640
        33.0       0.78      0.82      0.80       546
        34.0       0.76      0.85      0.80       282
        35.0       0.79      0.73      0.76       736
        36.0       0.79      0.76      0.77       958
        37.0       0.79      0.79      0.79      1143
        38.0       0.77      0.71      0.74       210
        39.0       0.83      0.84      0.83      1219
        40.0       0.69      0.71      0.70       873

    accuracy                           0.82     36098
   macro avg       0.81      0.78      0.79     36098
weighted avg       0.82      0.82      0.82     36098


Layer 12 - Accuracy: 81.23%
              precision    recall  f1-score   support

         0.0       0.92      0.97      0.95      6418
         1.0       0.65      0.42      0.51       183
         2.0       0.86      0.90      0.88      2079
         3.0       0.82      0.83      0.83       412
         4.0       0.74      0.77      0.75      1996
         5.0       0.81      0.86      0.83      1674
         6.0       0.87      0.86      0.86       878
         7.0       0.74      0.69      0.72       182
         8.0       0.83      0.76      0.80      1007
         9.0       0.76      0.68      0.72       527
        10.0       0.87      0.81      0.84       782
        11.0       0.89      0.91      0.90       422
        12.0       0.82      0.84      0.83       600
        13.0       0.84      0.81      0.82       317
        14.0       0.84      0.83      0.84       662
        15.0       0.78      0.81      0.80       397
        16.0       0.87      0.76      0.81       278
        17.0       0.93      0.71      0.81        73
        18.0       0.77      0.69      0.73       148
        19.0       0.71      0.63      0.67      1271
        20.0       0.81      0.52      0.63        25
        21.0       0.84      0.83      0.84       294
        22.0       0.86      0.67      0.75       111
        23.0       0.84      0.85      0.85       647
        24.0       0.86      0.79      0.82       528
        25.0       0.73      0.81      0.77       926
        26.0       0.61      0.76      0.68       482
        27.0       0.73      0.62      0.67      1394
        28.0       0.86      0.82      0.84       865
        29.0       0.83      0.85      0.84       604
        30.0       0.82      0.84      0.83      1202
        31.0       0.66      0.69      0.67      2107
        32.0       0.83      0.80      0.82       640
        33.0       0.84      0.77      0.80       546
        34.0       0.84      0.78      0.81       282
        35.0       0.80      0.72      0.76       736
        36.0       0.75      0.80      0.77       958
        37.0       0.79      0.71      0.75      1143
        38.0       0.82      0.64      0.72       210
        39.0       0.78      0.85      0.82      1219
        40.0       0.72      0.66      0.69       873

    accuracy                           0.81     36098
   macro avg       0.80      0.76      0.78     36098
weighted avg       0.81      0.81      0.81     36098


Layer 13 - Accuracy: 81.14%
              precision    recall  f1-score   support

         0.0       0.93      0.97      0.95      6418
         1.0       0.55      0.47      0.51       183
         2.0       0.88      0.89      0.88      2079
         3.0       0.81      0.82      0.82       412
         4.0       0.74      0.77      0.75      1996
         5.0       0.79      0.86      0.83      1674
         6.0       0.87      0.86      0.87       878
         7.0       0.72      0.68      0.69       182
         8.0       0.80      0.79      0.80      1007
         9.0       0.77      0.66      0.71       527
        10.0       0.84      0.82      0.83       782
        11.0       0.88      0.89      0.88       422
        12.0       0.83      0.84      0.84       600
        13.0       0.84      0.78      0.81       317
        14.0       0.84      0.82      0.83       662
        15.0       0.80      0.77      0.79       397
        16.0       0.88      0.76      0.82       278
        17.0       0.84      0.71      0.77        73
        18.0       0.79      0.68      0.73       148
        19.0       0.72      0.65      0.68      1271
        20.0       0.71      0.48      0.57        25
        21.0       0.86      0.84      0.85       294
        22.0       0.82      0.68      0.74       111
        23.0       0.86      0.84      0.85       647
        24.0       0.85      0.79      0.82       528
        25.0       0.78      0.77      0.77       926
        26.0       0.66      0.73      0.69       482
        27.0       0.75      0.59      0.66      1394
        28.0       0.83      0.85      0.84       865
        29.0       0.82      0.83      0.83       604
        30.0       0.80      0.82      0.81      1202
        31.0       0.65      0.73      0.69      2107
        32.0       0.83      0.81      0.82       640
        33.0       0.87      0.74      0.80       546
        34.0       0.85      0.75      0.80       282
        35.0       0.80      0.72      0.76       736
        36.0       0.74      0.79      0.76       958
        37.0       0.73      0.78      0.76      1143
        38.0       0.83      0.67      0.74       210
        39.0       0.81      0.83      0.82      1219
        40.0       0.72      0.65      0.68       873

    accuracy                           0.81     36098
   macro avg       0.80      0.76      0.78     36098
weighted avg       0.81      0.81      0.81     36098


Layer 14 - Accuracy: 85.82%
              precision    recall  f1-score   support

         0.0       0.93      0.97      0.95      6418
         1.0       0.66      0.39      0.49       183
         2.0       0.89      0.92      0.91      2079
         3.0       0.87      0.85      0.86       412
         4.0       0.83      0.85      0.84      1996
         5.0       0.87      0.86      0.87      1674
         6.0       0.91      0.88      0.90       878
         7.0       0.77      0.77      0.77       182
         8.0       0.87      0.81      0.84      1007
         9.0       0.83      0.76      0.80       527
        10.0       0.90      0.89      0.90       782
        11.0       0.91      0.90      0.91       422
        12.0       0.85      0.87      0.86       600
        13.0       0.88      0.80      0.84       317
        14.0       0.91      0.87      0.89       662
        15.0       0.88      0.84      0.86       397
        16.0       0.93      0.82      0.87       278
        17.0       0.94      0.86      0.90        73
        18.0       0.83      0.75      0.79       148
        19.0       0.77      0.78      0.78      1271
        20.0       0.77      0.68      0.72        25
        21.0       0.90      0.86      0.88       294
        22.0       0.74      0.78      0.76       111
        23.0       0.89      0.91      0.90       647
        24.0       0.85      0.87      0.86       528
        25.0       0.80      0.85      0.83       926
        26.0       0.72      0.80      0.76       482
        27.0       0.77      0.74      0.76      1394
        28.0       0.90      0.86      0.88       865
        29.0       0.84      0.90      0.87       604
        30.0       0.82      0.91      0.86      1202
        31.0       0.78      0.73      0.76      2107
        32.0       0.88      0.87      0.87       640
        33.0       0.84      0.86      0.85       546
        34.0       0.92      0.85      0.88       282
        35.0       0.86      0.76      0.81       736
        36.0       0.82      0.84      0.83       958
        37.0       0.81      0.81      0.81      1143
        38.0       0.88      0.75      0.81       210
        39.0       0.87      0.85      0.86      1219
        40.0       0.78      0.76      0.77       873

    accuracy                           0.86     36098
   macro avg       0.85      0.82      0.83     36098
weighted avg       0.86      0.86      0.86     36098


Layer 15 - Accuracy: 87.94%
              precision    recall  f1-score   support

         0.0       0.95      0.97      0.96      6418
         1.0       0.73      0.51      0.60       183
         2.0       0.92      0.93      0.93      2079
         3.0       0.89      0.86      0.87       412
         4.0       0.86      0.87      0.86      1996
         5.0       0.89      0.87      0.88      1674
         6.0       0.92      0.92      0.92       878
         7.0       0.85      0.77      0.81       182
         8.0       0.89      0.89      0.89      1007
         9.0       0.83      0.83      0.83       527
        10.0       0.92      0.91      0.91       782
        11.0       0.92      0.92      0.92       422
        12.0       0.88      0.88      0.88       600
        13.0       0.91      0.82      0.86       317
        14.0       0.92      0.90      0.91       662
        15.0       0.89      0.90      0.90       397
        16.0       0.89      0.89      0.89       278
        17.0       0.96      0.89      0.92        73
        18.0       0.86      0.77      0.81       148
        19.0       0.84      0.80      0.82      1271
        20.0       0.86      0.76      0.81        25
        21.0       0.92      0.88      0.90       294
        22.0       0.84      0.82      0.83       111
        23.0       0.91      0.91      0.91       647
        24.0       0.89      0.89      0.89       528
        25.0       0.83      0.87      0.85       926
        26.0       0.80      0.77      0.78       482
        27.0       0.78      0.80      0.79      1394
        28.0       0.90      0.89      0.89       865
        29.0       0.86      0.91      0.89       604
        30.0       0.87      0.90      0.89      1202
        31.0       0.78      0.79      0.79      2107
        32.0       0.90      0.90      0.90       640
        33.0       0.88      0.88      0.88       546
        34.0       0.91      0.89      0.90       282
        35.0       0.86      0.80      0.83       736
        36.0       0.81      0.88      0.85       958
        37.0       0.85      0.81      0.83      1143
        38.0       0.90      0.82      0.86       210
        39.0       0.86      0.89      0.87      1219
        40.0       0.86      0.78      0.82       873

    accuracy                           0.88     36098
   macro avg       0.87      0.85      0.86     36098
weighted avg       0.88      0.88      0.88     36098


Layer 16 - Accuracy: 88.36%
              precision    recall  f1-score   support

         0.0       0.95      0.97      0.96      6418
         1.0       0.70      0.54      0.60       183
         2.0       0.91      0.96      0.93      2079
         3.0       0.88      0.89      0.88       412
         4.0       0.86      0.85      0.85      1996
         5.0       0.88      0.87      0.87      1674
         6.0       0.91      0.92      0.92       878
         7.0       0.88      0.80      0.84       182
         8.0       0.91      0.87      0.89      1007
         9.0       0.87      0.81      0.84       527
        10.0       0.93      0.92      0.92       782
        11.0       0.94      0.94      0.94       422
        12.0       0.86      0.91      0.88       600
        13.0       0.93      0.83      0.87       317
        14.0       0.93      0.90      0.91       662
        15.0       0.89      0.90      0.90       397
        16.0       0.89      0.88      0.88       278
        17.0       0.96      0.92      0.94        73
        18.0       0.82      0.84      0.83       148
        19.0       0.83      0.78      0.81      1271
        20.0       0.91      0.80      0.85        25
        21.0       0.89      0.91      0.90       294
        22.0       0.86      0.83      0.84       111
        23.0       0.93      0.91      0.92       647
        24.0       0.87      0.91      0.89       528
        25.0       0.86      0.84      0.85       926
        26.0       0.82      0.84      0.83       482
        27.0       0.78      0.79      0.78      1394
        28.0       0.90      0.92      0.91       865
        29.0       0.86      0.91      0.89       604
        30.0       0.88      0.90      0.89      1202
        31.0       0.83      0.75      0.79      2107
        32.0       0.92      0.90      0.91       640
        33.0       0.88      0.89      0.88       546
        34.0       0.95      0.91      0.93       282
        35.0       0.88      0.80      0.84       736
        36.0       0.83      0.89      0.86       958
        37.0       0.81      0.85      0.83      1143
        38.0       0.85      0.87      0.86       210
        39.0       0.86      0.89      0.88      1219
        40.0       0.82      0.83      0.83       873

    accuracy                           0.88     36098
   macro avg       0.88      0.86      0.87     36098
weighted avg       0.88      0.88      0.88     36098


Layer 17 - Accuracy: 87.28%
              precision    recall  f1-score   support

         0.0       0.95      0.97      0.96      6418
         1.0       0.70      0.51      0.59       183
         2.0       0.94      0.92      0.93      2079
         3.0       0.86      0.89      0.87       412
         4.0       0.83      0.85      0.84      1996
         5.0       0.84      0.90      0.87      1674
         6.0       0.92      0.92      0.92       878
         7.0       0.80      0.75      0.77       182
         8.0       0.88      0.87      0.88      1007
         9.0       0.83      0.83      0.83       527
        10.0       0.90      0.93      0.92       782
        11.0       0.93      0.91      0.92       422
        12.0       0.88      0.86      0.87       600
        13.0       0.88      0.86      0.87       317
        14.0       0.88      0.91      0.89       662
        15.0       0.88      0.90      0.89       397
        16.0       0.84      0.86      0.85       278
        17.0       0.94      0.88      0.91        73
        18.0       0.86      0.82      0.84       148
        19.0       0.82      0.71      0.76      1271
        20.0       0.91      0.80      0.85        25
        21.0       0.91      0.89      0.90       294
        22.0       0.87      0.81      0.84       111
        23.0       0.91      0.92      0.91       647
        24.0       0.89      0.89      0.89       528
        25.0       0.84      0.84      0.84       926
        26.0       0.79      0.82      0.80       482
        27.0       0.79      0.76      0.77      1394
        28.0       0.88      0.90      0.89       865
        29.0       0.88      0.89      0.88       604
        30.0       0.87      0.88      0.88      1202
        31.0       0.78      0.75      0.77      2107
        32.0       0.92      0.90      0.91       640
        33.0       0.88      0.90      0.89       546
        34.0       0.92      0.90      0.91       282
        35.0       0.84      0.77      0.81       736
        36.0       0.82      0.87      0.85       958
        37.0       0.80      0.83      0.82      1143
        38.0       0.87      0.87      0.87       210
        39.0       0.87      0.88      0.87      1219
        40.0       0.81      0.79      0.80       873

    accuracy                           0.87     36098
   macro avg       0.86      0.85      0.86     36098
weighted avg       0.87      0.87      0.87     36098


Layer 18 - Accuracy: 85.29%
              precision    recall  f1-score   support

         0.0       0.95      0.97      0.96      6418
         1.0       0.73      0.49      0.58       183
         2.0       0.90      0.94      0.92      2079
         3.0       0.88      0.84      0.86       412
         4.0       0.84      0.78      0.81      1996
         5.0       0.84      0.88      0.86      1674
         6.0       0.90      0.90      0.90       878
         7.0       0.81      0.70      0.75       182
         8.0       0.86      0.84      0.85      1007
         9.0       0.79      0.80      0.80       527
        10.0       0.91      0.92      0.91       782
        11.0       0.92      0.89      0.91       422
        12.0       0.87      0.86      0.86       600
        13.0       0.92      0.78      0.84       317
        14.0       0.89      0.87      0.88       662
        15.0       0.86      0.84      0.85       397
        16.0       0.83      0.82      0.82       278
        17.0       0.88      0.79      0.83        73
        18.0       0.79      0.81      0.80       148
        19.0       0.73      0.72      0.72      1271
        20.0       0.88      0.60      0.71        25
        21.0       0.88      0.85      0.87       294
        22.0       0.85      0.72      0.78       111
        23.0       0.93      0.91      0.92       647
        24.0       0.83      0.89      0.86       528
        25.0       0.84      0.72      0.78       926
        26.0       0.78      0.78      0.78       482
        27.0       0.73      0.72      0.73      1394
        28.0       0.90      0.88      0.89       865
        29.0       0.85      0.90      0.87       604
        30.0       0.87      0.89      0.88      1202
        31.0       0.74      0.74      0.74      2107
        32.0       0.89      0.85      0.87       640
        33.0       0.84      0.86      0.85       546
        34.0       0.90      0.84      0.87       282
        35.0       0.83      0.76      0.79       736
        36.0       0.83      0.85      0.84       958
        37.0       0.72      0.86      0.78      1143
        38.0       0.86      0.79      0.82       210
        39.0       0.85      0.86      0.86      1219
        40.0       0.74      0.79      0.76       873

    accuracy                           0.85     36098
   macro avg       0.84      0.82      0.83     36098
weighted avg       0.85      0.85      0.85     36098


Layer 19 - Accuracy: 83.92%
              precision    recall  f1-score   support

         0.0       0.95      0.97      0.96      6418
         1.0       0.57      0.48      0.52       183
         2.0       0.90      0.92      0.91      2079
         3.0       0.82      0.85      0.84       412
         4.0       0.77      0.82      0.79      1996
         5.0       0.83      0.86      0.85      1674
         6.0       0.89      0.89      0.89       878
         7.0       0.86      0.71      0.78       182
         8.0       0.85      0.82      0.83      1007
         9.0       0.76      0.80      0.78       527
        10.0       0.87      0.90      0.89       782
        11.0       0.91      0.89      0.90       422
        12.0       0.82      0.85      0.83       600
        13.0       0.87      0.81      0.84       317
        14.0       0.86      0.85      0.85       662
        15.0       0.84      0.85      0.84       397
        16.0       0.79      0.79      0.79       278
        17.0       0.87      0.74      0.80        73
        18.0       0.76      0.74      0.75       148
        19.0       0.76      0.65      0.70      1271
        20.0       1.00      0.56      0.72        25
        21.0       0.90      0.87      0.88       294
        22.0       0.81      0.76      0.78       111
        23.0       0.91      0.89      0.90       647
        24.0       0.84      0.86      0.85       528
        25.0       0.80      0.80      0.80       926
        26.0       0.74      0.76      0.75       482
        27.0       0.68      0.70      0.69      1394
        28.0       0.87      0.89      0.88       865
        29.0       0.86      0.87      0.87       604
        30.0       0.85      0.87      0.86      1202
        31.0       0.73      0.69      0.71      2107
        32.0       0.88      0.83      0.86       640
        33.0       0.85      0.79      0.82       546
        34.0       0.88      0.87      0.87       282
        35.0       0.82      0.74      0.78       736
        36.0       0.79      0.83      0.81       958
        37.0       0.78      0.78      0.78      1143
        38.0       0.85      0.80      0.82       210
        39.0       0.81      0.85      0.83      1219
        40.0       0.77      0.70      0.73       873

    accuracy                           0.84     36098
   macro avg       0.83      0.80      0.81     36098
weighted avg       0.84      0.84      0.84     36098


Layer 20 - Accuracy: 81.92%
              precision    recall  f1-score   support

         0.0       0.96      0.96      0.96      6418
         1.0       0.54      0.41      0.47       183
         2.0       0.86      0.92      0.89      2079
         3.0       0.83      0.83      0.83       412
         4.0       0.74      0.77      0.75      1996
         5.0       0.83      0.81      0.82      1674
         6.0       0.89      0.88      0.88       878
         7.0       0.79      0.76      0.77       182
         8.0       0.80      0.80      0.80      1007
         9.0       0.75      0.72      0.74       527
        10.0       0.89      0.87      0.88       782
        11.0       0.85      0.89      0.87       422
        12.0       0.84      0.81      0.83       600
        13.0       0.87      0.81      0.84       317
        14.0       0.84      0.83      0.84       662
        15.0       0.81      0.82      0.82       397
        16.0       0.81      0.71      0.76       278
        17.0       0.85      0.73      0.79        73
        18.0       0.72      0.74      0.73       148
        19.0       0.69      0.64      0.67      1271
        20.0       0.85      0.44      0.58        25
        21.0       0.88      0.82      0.85       294
        22.0       0.74      0.67      0.70       111
        23.0       0.89      0.90      0.90       647
        24.0       0.83      0.82      0.83       528
        25.0       0.79      0.75      0.77       926
        26.0       0.70      0.76      0.73       482
        27.0       0.65      0.68      0.66      1394
        28.0       0.89      0.87      0.88       865
        29.0       0.83      0.89      0.86       604
        30.0       0.84      0.85      0.85      1202
        31.0       0.71      0.65      0.68      2107
        32.0       0.85      0.85      0.85       640
        33.0       0.82      0.77      0.79       546
        34.0       0.83      0.86      0.84       282
        35.0       0.78      0.71      0.74       736
        36.0       0.72      0.85      0.78       958
        37.0       0.74      0.77      0.76      1143
        38.0       0.81      0.74      0.77       210
        39.0       0.81      0.83      0.82      1219
        40.0       0.68      0.72      0.70       873

    accuracy                           0.82     36098
   macro avg       0.80      0.78      0.79     36098
weighted avg       0.82      0.82      0.82     36098


Layer 21 - Accuracy: 80.76%
              precision    recall  f1-score   support

         0.0       0.95      0.97      0.96      6418
         1.0       0.51      0.34      0.41       183
         2.0       0.89      0.88      0.89      2079
         3.0       0.83      0.83      0.83       412
         4.0       0.70      0.77      0.73      1996
         5.0       0.82      0.84      0.83      1674
         6.0       0.85      0.88      0.87       878
         7.0       0.82      0.71      0.76       182
         8.0       0.78      0.81      0.80      1007
         9.0       0.65      0.77      0.71       527
        10.0       0.88      0.86      0.87       782
        11.0       0.86      0.91      0.89       422
        12.0       0.83      0.79      0.81       600
        13.0       0.86      0.81      0.83       317
        14.0       0.85      0.83      0.84       662
        15.0       0.79      0.80      0.79       397
        16.0       0.74      0.70      0.72       278
        17.0       0.82      0.77      0.79        73
        18.0       0.76      0.73      0.74       148
        19.0       0.69      0.57      0.62      1271
        20.0       0.79      0.60      0.68        25
        21.0       0.88      0.78      0.83       294
        22.0       0.65      0.64      0.64       111
        23.0       0.88      0.90      0.89       647
        24.0       0.80      0.78      0.79       528
        25.0       0.78      0.74      0.76       926
        26.0       0.71      0.63      0.67       482
        27.0       0.64      0.66      0.65      1394
        28.0       0.84      0.89      0.86       865
        29.0       0.79      0.90      0.84       604
        30.0       0.82      0.85      0.83      1202
        31.0       0.71      0.60      0.65      2107
        32.0       0.83      0.80      0.81       640
        33.0       0.78      0.76      0.77       546
        34.0       0.80      0.86      0.83       282
        35.0       0.73      0.71      0.72       736
        36.0       0.76      0.76      0.76       958
        37.0       0.73      0.79      0.76      1143
        38.0       0.80      0.70      0.74       210
        39.0       0.80      0.84      0.82      1219
        40.0       0.67      0.69      0.68       873

    accuracy                           0.81     36098
   macro avg       0.78      0.77      0.77     36098
weighted avg       0.81      0.81      0.81     36098


Layer 22 - Accuracy: 79.43%
              precision    recall  f1-score   support

         0.0       0.95      0.96      0.96      6418
         1.0       0.53      0.28      0.37       183
         2.0       0.82      0.92      0.87      2079
         3.0       0.80      0.82      0.81       412
         4.0       0.74      0.71      0.72      1996
         5.0       0.81      0.81      0.81      1674
         6.0       0.85      0.87      0.86       878
         7.0       0.78      0.67      0.72       182
         8.0       0.82      0.71      0.76      1007
         9.0       0.70      0.72      0.71       527
        10.0       0.86      0.85      0.85       782
        11.0       0.86      0.90      0.88       422
        12.0       0.81      0.80      0.80       600
        13.0       0.84      0.78      0.81       317
        14.0       0.82      0.80      0.81       662
        15.0       0.78      0.80      0.79       397
        16.0       0.70      0.69      0.70       278
        17.0       0.80      0.59      0.68        73
        18.0       0.75      0.60      0.67       148
        19.0       0.63      0.61      0.62      1271
        20.0       0.71      0.40      0.51        25
        21.0       0.83      0.77      0.80       294
        22.0       0.62      0.54      0.58       111
        23.0       0.88      0.88      0.88       647
        24.0       0.80      0.80      0.80       528
        25.0       0.75      0.73      0.74       926
        26.0       0.69      0.62      0.66       482
        27.0       0.64      0.64      0.64      1394
        28.0       0.81      0.87      0.84       865
        29.0       0.79      0.88      0.83       604
        30.0       0.82      0.83      0.82      1202
        31.0       0.66      0.62      0.64      2107
        32.0       0.84      0.79      0.82       640
        33.0       0.71      0.79      0.75       546
        34.0       0.79      0.77      0.78       282
        35.0       0.71      0.65      0.68       736
        36.0       0.72      0.76      0.74       958
        37.0       0.70      0.77      0.73      1143
        38.0       0.77      0.75      0.76       210
        39.0       0.80      0.84      0.82      1219
        40.0       0.65      0.63      0.64       873

    accuracy                           0.79     36098
   macro avg       0.76      0.74      0.75     36098
weighted avg       0.79      0.79      0.79     36098


Layer 23 - Accuracy: 77.41%
              precision    recall  f1-score   support

         0.0       0.95      0.96      0.96      6418
         1.0       0.43      0.30      0.35       183
         2.0       0.84      0.88      0.86      2079
         3.0       0.79      0.75      0.77       412
         4.0       0.73      0.63      0.68      1996
         5.0       0.83      0.78      0.80      1674
         6.0       0.86      0.81      0.84       878
         7.0       0.79      0.66      0.72       182
         8.0       0.70      0.81      0.75      1007
         9.0       0.72      0.68      0.70       527
        10.0       0.86      0.82      0.84       782
        11.0       0.79      0.91      0.84       422
        12.0       0.78      0.80      0.79       600
        13.0       0.83      0.80      0.81       317
        14.0       0.79      0.80      0.80       662
        15.0       0.75      0.75      0.75       397
        16.0       0.71      0.61      0.66       278
        17.0       0.76      0.53      0.63        73
        18.0       0.74      0.67      0.70       148
        19.0       0.62      0.58      0.60      1271
        20.0       1.00      0.28      0.44        25
        21.0       0.72      0.82      0.77       294
        22.0       0.64      0.42      0.51       111
        23.0       0.88      0.86      0.87       647
        24.0       0.81      0.77      0.79       528
        25.0       0.60      0.82      0.69       926
        26.0       0.62      0.69      0.65       482
        27.0       0.67      0.53      0.59      1394
        28.0       0.79      0.88      0.83       865
        29.0       0.75      0.89      0.82       604
        30.0       0.80      0.82      0.81      1202
        31.0       0.57      0.68      0.62      2107
        32.0       0.73      0.85      0.78       640
        33.0       0.73      0.75      0.74       546
        34.0       0.81      0.73      0.77       282
        35.0       0.71      0.60      0.65       736
        36.0       0.72      0.75      0.74       958
        37.0       0.76      0.51      0.61      1143
        38.0       0.78      0.65      0.71       210
        39.0       0.73      0.87      0.79      1219
        40.0       0.70      0.55      0.62       873

    accuracy                           0.77     36098
   macro avg       0.75      0.71      0.72     36098
weighted avg       0.78      0.77      0.77     36098


